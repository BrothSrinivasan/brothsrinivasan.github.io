---
title: "A Look into the Ideologies of Supreme Court Justices"
author: "Barath Srinivasan"
date: "May 22, 2019"
output: html_document
---

```{r setup, 1, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

# **Introduction**
Like most Americans, you probably watched President Trump's recent Supreme Court nominations and had certain personal reservations about the nomination, whether it be left-leaning or right-leaning. But regardless of what aisle of the political spectrum you stand, you can’t disagree on the immense difficulty that United States Senators face in confirming a nomination. One of the reasons for this difficulty lies in the nominees’ unwillingness to display their true beliefs before the Senators. On one hand, you can’t blame them as they want to appear independent to ensure the integrity of the Supreme Court if they do get approval from the Senate. That being said, I would argue a truly independent Justice is one that can acknowledge his/her own biases, but show ways in which he/she has worked towards mitigating its influence. Unfortunately, because nominees are unlikely to do that I began wondering if we could determine the ideological leaning of a particular nominee based on how they have dealt with cases from different issue areas. To that effect, let's take a stroll down the data science pipeline to see what we can determine and produce.

# **Libraries**
For this walkthrough, we are going to be using R. And before we continue, we are going to need to download certain libraries in order to be able to take full advantage of R.

```{r import, 6}
library(readr);
library(dplyr);
library(tidyr);
library(ggplot2);
library(broom);
library(caret);
library(shiny);
```

# **Loading/Preparing Data**

### Determining Datasets
Now that we have the preliminary work out of the way, we are going to need to find appropriate datasets in order to be able to make our analysis.

Fortunately, for us there is a database by Washington University’s School of Law called the Supreme Court Database. This database has information on all the cases that was brought before the Court. But University of Washington provides many different types of datasets that provide different kinds of information, so choosing the right one is going to be an important step. Firstly, we are going to choose their Justice Centered Data as this dataset provides information of each individual Justice. Second, we are also going to choose from their modern database, which contains information about the Justices from the 1946 till present. The reason we are choosing this particular time period, instead of choosing all justices since the birth of America is because a lot of issues that were considered liberal in the 1800s are no longer considered liberal and vice versa. So, we won’t be able to gain a true measure by including all the Justices since meaning differs based on the era we look at. Finally, our dataset is going to their most recent release, in order to have stats that are based on up-to-date information.

```{r, justice_data, 1}
justice_info <- read_csv("Justice_Info.csv")
```

We are almost done at this point in choosing our dataset; however, we are missing a single piece of information that we are going to need later. That information is a classification of whether a Justice is considered liberal or conservative. This is because the Washington University dataset provides information about Justices as it pertains to each case brought before the Court. The dataset we require is one that tells us whether during the tenure of a Justice they were considered liberal or conservative. For this information we turn to a dataset provided by two professors from University of Michigan, Andrew Martin and Kevin Quinn. These two professors have compiled the cases of the Court and assigned a ideological score to each Justice over their tenure, where a positive score means the Justice is more conservative and a negative score means the Justice is more liberal. It is important to note that Martin and Quinn have not answered our question as to whether we can use different issue areas to determine whether a nominee is conservative or liberal because, according to Professor Farnsworth of University of Texas’s School of Law, “the authors' model treats all cases as equally important and revealing”. The problem is that not all cases are equal since Justices are known to have different standing for each kind of matter brought before the Court. And again we are going to choose the justice centered dataset and their latest version.

```{r, score_data, 1}
justice_idea <- read_csv("justices.csv")
```

### Tidying Our Dataset
Now we that we have our datasets but we have two problems. One, because our datasets have too much information, they will be impossible to understand in their current stage. Two, we have two different datasets and neither will be useful unless we find a way to combine them.

First thing we are going to do is find the best suitable variables from our justice dataset. To aid us with that, University of Washington provides a documentation to help interpret their dataset. And by going through the documentation, there seem to be certain variables that could be pertinent to our analysis.

- caseID – a unique ID that distinguishes the cases
- term – the year the case was decided
- chief – the Chief Justice for that particular year
- justice – an ID that is unique to each Justice
- justiceName – the name of the Justices
- issueArea – the group to which that particular case belongs in (total of 14 areas)
- decisionDirection – the direction (conservative or liberal) the Court’s final decision leans toward
- direction – the direction (conservative or liberal) each Justice’s vote leans towards

So, we are going to filter out these specific columns. Then we are going to rename justice, decisionDirection, and direction to justiceID, courtDecision and justiceDirection, respectively, for clarity. Also, we are going call the factor function on the two direction variables because while R considers them to be numeric variables, we know that the directions are categorical by nature and not numeric.
```{r, tidying_data_p1, 8}
justice_clean <-
  justice_info %>%
  select(caseId, term, chief, justiceID=justice, justiceName, issueArea, 
    courtDirection=decisionDirection, justiceDirection=direction) %>%
  filter(!is.na(justiceDirection))

justice_clean$courtDirection <- ifelse(justice_clean$courtDirection == 1, 0, 1)
justice_clean$justiceDirection <- ifelse(justice_clean$justiceDirection == 1, 0, 1)
justice_clean$courtDirection <- factor(justice_clean$courtDirection);
justice_clean$justiceDirection <- factor(justice_clean$justiceDirection);

justice_clean %>% head()
```

Next we are trying to distill important information from the score dataset. And after taking a look, it seems we only need a couple of the columns.

- term – the year the score was assigned
- justice – an ID that is unique to each Justice
- post_mn – the score of the Justice
- post_sd – the standard deviation of the score

So, like before we are going to filter out theses specific columns. Then we are going to rename justice, post_mn and post_sd to justiceID, score_mean, and score_sd, respectively, for personal clarity.


```{r, tidying_data_p2, 4}
justice_spread <-
  justice_idea %>%
  select(term, justiceID=justice, score_mean=post_mn, score_sd=post_sd)

justice_spread %>% head()
```

Finally, we are going to combine these two new concise datasets by using an inner join on term and justiceID. By doing this, we can create a new column in the new justice dataset that pertains to score and the standard deviation of said score.

```{r, joining_data, 4}
justice_combined <-
  justice_clean %>%
  inner_join(justice_spread)

justice_combined %>% head()
```

# **Data Analysis**

### Visualizing Data
At this point, we have cleaned our data. However, at this point the data is still numbers in a table. We need to convert it to visuals in order to truly to see if there is some sort of trend for us to continue.

So, we begin our visual analysis by graphing the total number of liberal decision by the Court and total number of conservative decisions by the Court over the years. Then we finish by drawing regression lines over them to see any trends.

```{r, graph_p1, 6}
justice_combined %>%
  group_by(term, courtDirection) %>%
  summarize(no_of_votes=n()) %>%
  ggplot(mapping=aes(x=term, y=no_of_votes, color=courtDirection)) +
    geom_point() +
    geom_smooth()
```

Unfortunately, this analysis is neither easy to understand nor fruitful in anyway. Even though there is a downward trend, you have to realize the number of cases the Court took decreased over the years which could be the cause of the trend. This leads to the next theory that maybe by looking at each specific time periods of the data, we can determine something. In our case, we are going to split up the data based on the Chief Justice. In my opinion, the best way would be to create a function so, we can switch between Chief Justices without having to recode our pipeline. Also, it avoids cramping all the data together by displaying all of the groups.

```{r, graph_p2, 11}
eachCourt <- function(justice) {
  justice_combined %>%
    select(term, chief, justiceName, score_mean, score_sd) %>%
    filter(chief == justice) %>%
    distinct() %>%
    ggplot(mapping=aes(x=factor(term), y=score_mean,
      ymax=score_mean+score_sd, ymin=score_mean-score_sd, color=justiceName)) +
      geom_pointrange() +
      xlab("year") +
      ylab("average MQ score")
}

eachCourt("Vinson") # an older court
eachCourt("Roberts") # a newer court
```

These graph are slightly better than the previous ones. They gives us an understanding of how more ideological an older Court like Vinson’s Court is versus a newer one like Robert’s Court. But they still aren't giving us any sort of useful information that we want. So, maybe we need to go down to the level of each individual Justice to see a trend. I propose creating a visual based on splitting their votes over the different issue areas.

```{r, graph_p3, 11}
eachJustice <- function(name) {
  justice_combined %>%
    select(term, justiceName, issueArea, justiceDirection) %>%
    filter(justiceName == name, !is.na(justice_combined$issueArea)) %>%
    ggplot(mapping=aes(x=factor(issueArea), y=justiceDirection, color=justiceDirection)) +
      geom_jitter() +
      theme(axis.text.x = element_text(angle=90)) +
      xlab("area of issue") +
      ylab("direction of vote")
}

eachJustice("AScalia") # a known conversative justice
eachJustice("RBGinsburg") # a known liberal justice
```

These graph are closer to what we want. We can see clear differences in certain issue areas between a liberal and conservative Justice. They are starting to give credence to the hypothesis that issue areas are correlated to overall ideological leaning. However, we are losing key information like how voting changed over their tenure in office. Also, the clutter makes it hard to tell at times which direction has more votes. We can fix that by making a bar graph based on the count of the votes. However, this time we will graph over time and then facet the graph by issue area to gain a understanding of how voting changed over time on each issue.

```{r, graph_p4, 12}
eachJustice <- function(name) {
  justice_combined %>%
    select(term, justiceName, issueArea, justiceDirection) %>%
    filter(justiceName == name, !is.na(justice_combined$issueArea)) %>%
    ggplot(mapping=aes(x=factor(term), fill=justiceDirection)) +
      geom_bar() +
      facet_wrap(~issueArea, ncol=3, scales="free_y") +
      theme(axis.text.x = element_text(angle=90)) +
      xlab("year") +
      ylab("voting count")
}

eachJustice("AScalia") # a known conversative justice
eachJustice("RBGinsburg") # a known liberal justice
```

Finally, we have graphs that are helpful in guiding our understanding. It is clear that certain issues clearly delineate whether a Justice is conservative or not. For example, the third issue group, which deals with the First Amendment, has more conservative votes by conservative Justices and more liberal votes among liberal Justices. However, issue groups eleven, thirteen, and fourteen don’t seem as important because they occur so rarely and probably don’t have to be considered in our modeling. With this knowledge, we can move onto the coolest step: machine learning.

# **Predictive Model**

### Preprocessing Data
Before we can create our model, we need to do some preliminary work.

First, we are going to need to format our table in a more convenient manner; we need to make each row correspond to a particular observance. We do this by creating a table which states for each year and Justice, whether they were more conservative or liberal on a particular issue area. Then we call upon the spread function to make each issue area its own column. We make these columns’ values being whether the Justice for that year was more conservative or more liberal on that issue area. Then we inner join this new table with the classification we gain from the Martin and Quinn. Finally, we factor the labels to make sure R considers them to categorical variables rather than numeric ones.

```{r, processing, 27}
justice_matrix <-
  justice_combined %>%
    group_by(justiceID, term, issueArea, justiceDirection) %>%
    summarize(directional_votes=n()) %>%
    ungroup() %>%
    inner_join(
      justice_combined %>%
        group_by(justiceID, term, issueArea) %>%
        summarise(total_votes=n()) %>%
        ungroup()
    ) %>%
  filter(!is.na(issueArea)) %>%
  mutate(direction=directional_votes/total_votes) %>%
  mutate(justiceID_year=paste(justiceID, "-", term)) %>%
  mutate(justiceDirection=ifelse(justiceDirection==0,1,2)) %>%
  group_by(justiceID_year, issueArea) %>%
  filter(direction == max(direction)) %>%
  filter(direction != 0.5) %>%
  ungroup() %>%
  select(justiceID_year, issueArea, justiceDirection) %>%
  spread(key=issueArea, value=justiceDirection) %>%
  replace(is.na(.),0) %>%
  inner_join(
    justice_spread %>%
      mutate(label=ifelse(score_mean > 0, 0, 1)) %>%
      mutate(justiceID_year=paste(justiceID, "-", term)) %>%
      select(justiceID_year, label)
  ) %>%
  select(-justiceID_year)

justice_matrix$label <- factor(justice_matrix$label);
```

Next, we are going to start my partitioning our dataset, so we may use part of it later for testing purposes later.

```{r, partioning, 4}
set.seed(100)
indices <- createDataPartition(justice_matrix$label, p=0.70, list = F)
trainData <- justice_matrix[indices, ]
testData <- justice_matrix[-indices, ]
```

Then we are going to down sample our observance in order to have the same number of observances of liberal labels vs conservative labels. This is going to ensure our model doesn’t become biased to either label over the other. Currently, our our observances stand at 259 observances for conservative vs 199 for liberal. However, with downsampling, we will have 199 observances for both conservatives and liberals.

```{r, downsampling, 2}
set.seed(100)
justice_balanced <- downSample(x = trainData[, c(T,T,T,T,T,T,T,T,T,T,T,T,T,T,F)],
  y = factor(trainData$label));
```

### Creating Model
Now, with everything set we can turn our attention to creating our model.

For our model we are going to exclude the eleventh, thirteenth, and fourteenth group because our visual analysis shows they are not to as significant as the other areas.

```{r, modeling_p1, 4}
model_one <- glm(Class~
  `1` + `10` + `12` + `2` + `3` + `4` + `5` + `6` + `7` + `8` + `9`,
  family='binomial', data=justice_balanced);

summary(model_one)
```

First of all, from this summary we can see that the model is a good fit since the deviance of residuals is close to zero. Second, we can see most of the areas are significant except areas four, six and seven. This means that we can reject the null hypothesis that there is no relationship between the areas and ideological leaning. However, lets try creating a new model but without the significant groups to see if we can improve our results.

```{r, modeling_p2, 4}
model_two <- glm(Class~
  `1` + `10` + `12` + `2` + `3` + `5` + `8` + `9`,
  family='binomial', data=justice_balanced);

summary(model_two)
```

Again, from this second summary, we see the same good fit from the deviance of residuals. Moreover, this time all the areas are significant. Now, to test if this second model is a better fit than the first model.

```{r, good_fit_test, 1}
anova(model_one, model_two, test ="Chisq")
```

It seems based on the p-value that the second model is not as significant and that means we fail to reject the null hypothesis. So, for now we will stick to the first model.

### Testing Model
Let’s test our model to see what the accuracy of our model is by using the test data that we procured before.

```{r, testing, 5}
predictions <- predict(model_one, newdata = testData, type = "response")
classify <- ifelse(predictions >= 0.50, 1, 0)
classify <- factor(classify)
accuracy <- mean(classify == testData$label)
```

It seems our model can predict with an accuracy of `r round(accuracy * 100, digits = 2)`%

# **Presenting Results**

### Deploying an App
Now that we have all our information and our models created, we need a way to present out findings. R provides us the ability to create an app by using its shiny library. It requires us to create an UI for displaying the graphics. Then we need to create a server in order to compute the logic based on the given input. So for this example, I have decided to plot the fitted values of my model and allow the user to manipulate area fields to determine where their point would land on the graph. Then, I have another output that displays the actual probability value of the user point being either liberal or conservative.

```{r, shiny}

shinyApp(
  ui = fluidPage(
    titlePanel("Is the Nominee Conservative or Liberal???"),
    sidebarLayout(
      sidebarPanel(
        selectInput("a1", "area 1 - Criminal Procedure", choices=c("Conservative", "Liberal")),
        selectInput("a2", "area 2 - Civil Rights", choices=c("Conservative", "Liberal")),
        selectInput("a3", "area 3 - First Amendment", choices=c("Conservative", "Liberal")),
        selectInput("a4", "area 4 - Due Process", choices=c("Conservative", "Liberal")),
        selectInput("a5", "area 5 - Privacy", choices=c("Conservative", "Liberal")),
        selectInput("a6", "area 6 - Attorneys", choices=c("Conservative", "Liberal")),
        selectInput("a7", "area 7 - Unions", choices=c("Conservative", "Liberal")),
        selectInput("a8", "area 8 - Economic Activity", choices=c("Conservative", "Liberal")),
        selectInput("a9", "area 9 - Judicial Power", choices=c("Conservative", "Liberal")),
        selectInput("a10", "area 10 - Federalism", choices=c("Conservative", "Liberal")),
        selectInput("a12", "area 12 - Federal Taxation", choices=c("Conservative", "Liberal"))
      ),
      mainPanel(
        plotOutput("regress"),
        textOutput("prob")
      )
    )
  ),
  server = function(input, output){
    
    plotPoints <-
      justice_balanced %>%
      select(Class) %>%
      mutate(response=model_one$fitted.value) %>%
      mutate(isNew=F) %>%
      sample_frac(0.5)

    
    output$regress <- renderPlot({
      newRow <- data.frame(
        `1`=ifelse(input$a1 == "Conservative", 1, 2),
        `2`=ifelse(input$a2 == "Conservative", 1, 2),
        `3`=ifelse(input$a3 == "Conservative", 1, 2),
        `4`=ifelse(input$a4 == "Conservative", 1, 2),
        `5`=ifelse(input$a5 == "Conservative", 1, 2),
        `6`=ifelse(input$a6 == "Conservative", 1, 2),
        `7`=ifelse(input$a7 == "Conservative", 1, 2),
        `8`=ifelse(input$a8 == "Conservative", 1, 2),
        `9`=ifelse(input$a9 == "Conservative", 1, 2),
        `10`=ifelse(input$a10 == "Conservative", 1, 2),
        `12`=ifelse(input$a12 == "Conservative", 1, 2),
        label=NA,
        check.names=F
      )
      
      newRow <-
        data_frame(response=predict(model_one, newdata = newRow, type = "response")) %>%
          mutate(isNew=T) %>%
          mutate(Class=ifelse(response >= 0.50, 1, 0))
      
      combined <-
        rbind(plotPoints, newRow) %>%
        arrange(Class, response) %>%
        tibble::rowid_to_column()
      
      combined %>%
       ggplot(mapping=aes(x=rowid, y=response, color=factor(Class))) +
        geom_point(show.legend=F) +
        geom_point(data=combined %>% filter(isNew == T),
          aes(x=rowid, y=response, color="userPoint"), size=5, show.legend=F) +
        scale_color_manual(values=c("0"="red","1"="blue", "userPoint"="purple")) +
        xlab("rank") +
        ylab("probability")
    })
    
    output$prob <- renderText({
      newRow <- data.frame(
        `1`=ifelse(input$a1 == "Conservative", 1, 2),
        `2`=ifelse(input$a2 == "Conservative", 1, 2),
        `3`=ifelse(input$a3 == "Conservative", 1, 2),
        `4`=ifelse(input$a4 == "Conservative", 1, 2),
        `5`=ifelse(input$a5 == "Conservative", 1, 2),
        `6`=ifelse(input$a6 == "Conservative", 1, 2),
        `7`=ifelse(input$a7 == "Conservative", 1, 2),
        `8`=ifelse(input$a8 == "Conservative", 1, 2),
        `9`=ifelse(input$a9 == "Conservative", 1, 2),
        `10`=ifelse(input$a10 == "Conservative", 1, 2),
        `12`=ifelse(input$a12 == "Conservative", 1, 2),
        label=NA,
        check.names=F
      )
      
      response=predict(model_one, newdata = newRow, type = "response");
      probab <- round(response * 100, digits=3);
      
      paste("The probability of this observance being liberal is ", probab, "%. ",
       "The probability of this observance being conservative is ", 100-probab, "%.", sep="")
    })
  }
)
```

Once we code the the shiny app, we have then deploy the app using shinyapps.io. Unfortunately, just knitting the code into an HTML will produce a static version of the app, like what you see above. You need to create a login and follow the instructions that pop up. Afterwards, if everything goes correctly you will have an app just like [this](https://brothsrinivasan.shinyapps.io/320Final/).

# **Conclusion**

### Final Thoughts
Overall, we can see that by using how nominees have acted in certain areas in the past, we can determine with a 90% accuracy whether or not they are left leaning or right leaning. It means that Senators can use something similar to this model to figure out whether a nominee truly believes what they say they believe.

### Improvements
Some ways that we could potentially improve on this model is include by more including more groups like independent, moderately liberal, and moderately conservative. This would help to show a difference between someone who is only slight right leaning vs someone who is extremely conservative.

### Further Resources
If you are interested in further looking at the topic of Supreme Court nominations, I suggest you take a look at [this website](https://aflcio.org/2016/8/30/why-supreme-court-nominations-are-one-most-important-issues-working-people) for a starting place. Also, I also suggest you read [this journal entry](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1000986) because it is the same entry that allowed me to find certain weaknesses in the Martin-Quinn Score. Finally, if you are interested in improving on this model based on my suggested improvements, I suggest you visit [this tutorial](https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/) from UCLA that takes you step-by-step on how to do multi class logistic regression, which is also done in R.

### Citations
- [Case/Justice Dataset](http://scdb.wustl.edu/data.php)
- [Case/Justice Handbook](http://scdb.wustl.edu/documentation.php?s=2)
- [Martin-Quinn Dataset and Handbook](https://mqscores.lsa.umich.edu/measures.php)