---
title: "A Look into the Ideologies of Supreme Court Justices"
author: "Barath Srinivasan"
date: "May 22, 2019"
output: html_document
---

```{r setup, 1, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

# **Introduction**
Like most Americans, you probably watched the President Trump's recent Supreme Court nominations and had certain personal reservations about the nomination, whether it be left-leaning or right-leaning. But regardless of what aisle of the political spectrum you stand, you can’t disagree on the immense difficulty that United States Senators face in confirming a nomination. One of the reasons for this difficulty lies in the nominees’ unwillingness to display their true beliefs about before the Senators. On one hand, you can’t blame them as they want to appear to remain independent to ensure the integrity of the Supreme Court if they do get approval from the Senate. That being said, I would argue a truly independent Justice is one that can acknowledge his/her own biases, but show ways in which he/she has worked towards mitigating its influence. Unfortunately, because nominees are not unlikely to do that I began wondering if we could determine the ideological leaning of a particular nominee based on how they have dealt with cases from different issue areas. To that affect, lets take a stroll down the data science pipeline to see what we can determine and produce.

# **Libraries**
For this walkthrough, we are going to be using R. And before we continue, we are going to need to download certain libraries in order to be able to use R to its fullest. 

```{r import, 6}
library(readr);
library(dplyr);
library(tidyr);
library(ggplot2);
library(broom);
library(caret);
```

# **Loading/Preparing Data**

### Determining Datasets
Now that we have the preliminary work out of the way, we are going to need to find appropriate datasets in order to be able to make our analysis.

Fortunately, for us there is a database by Washington University’s School of Law called the Supreme Court Database. This database has information on all the cases that was brought before the Court. But University of Washington provides many different types of datasets that provide different kind of information, so choosing the right one is going to be an important step. Firstly, we are going to choose their Justice Centered Data as this dataset provides information of each individual Justice. Second, we are also going to chose from their modern database, which contains information about the Justices from the 1946 till present. The reason we are choosing this particular time period, instead of choosing all justices since the birth of America is because a lot of issues that were considered liberal in the 1800s are no longer considered liberal and vice versa. So, we won’t be able to gain a true measure by including all the Justices since meaning differs based on the era we look at. Finally, our dataset is going to their most recent release, in order to have stats that are based on up-to-date information.

```{r, justice_data, 1}
justice_info <- read_csv("~/Desktop/320 Final/Justice_Info.csv")
```

We are almost done at this point in choosing our dataset; however, we are missing a single piece of information that we are going to need later. That information is a classification of whether a Justice is considered liberal or conservative. This is because the Washington University dataset provides information about Justice as it pertains to each case brought before the Court. The dataset we require is one that tells us whether during the tenure of a Justice whether they were considered liberal or conservative. For this information we turn to a dataset provided by two professors from University of Michigan, Andrew Martin and Kevin Quinn. These two professors have compiled the cases of the Court and assigned a ideological score to each Justice over their tenure, where a positive score means the Justice is more conservative and a negative score means the Justice is more liberal. It is important to note that Martin and Quinn have not answered our question as to whether we can use different issue areas to determine whether a nominee is conservative or liberal because, according to Professor Farnsworth of University of Texas’s School of Law, “the authors' model treats all cases as equally important and revealing”. The problem is that not all cases are equal since Justices are known to have different standing for each kind of matter brought before the Court. And again we are going to choose the justice centered dataset and their latest version.

```{r, score_data, 1}
justice_idea <- read_csv("justices.csv")
```

### Tidying Our Dataset
Now we are our datasets but we have two problems. One, our datasets has too much information that it will be impossible to understand it in its current stage. Two, we have two different datasets and neither will be useful unless we find a way to combine them.

First thing we are going to do is find the best suitable variables from our justice dataset. To aid us with that University of Washington provides a documentation to help interpret their dataset. And by going though the documentation, there seem to be certain variables that could be pertinent to our analysis.
- caseID – a unique ID that distinguishes the cases
- term – the year the case was decided
- chief – the chief justice for that particular year
- justice – an ID that is unique to each Justice
- justiceName – the name of the Justices
- issueArea – the group to which that particular case belongs in (total of 14 areas)
- decisionDirection – the direction (conservative or liberal) the Court’s final decision leans toward
- direction – the direction (conservative or liberal) each Justice’s vote leans towards

So, we are going to filter out these specific columns. Then we are going to rename justice, decisionDirection, and direction to justiceID, courtDecision and justiceDirection, respectively, for clarity. Also, we are going to factor the two direction variables because while R considers them to be numeric variables, we know that the directions are categorical by nature and not numeric.
```{r, tidying_data_p1, 8}
justice_clean <-
  justice_info %>%
  select(caseId, term, chief, justiceID=justice, justiceName, issueArea, 
    courtDirection=decisionDirection, justiceDirection=direction) %>%
  filter(!is.na(justiceDirection))

justice_clean$courtDirection <- ifelse(justice_clean$courtDirection == 1, 0, 1)
justice_clean$justiceDirection <- ifelse(justice_clean$justiceDirection == 1, 0, 1)
justice_clean$courtDirection <- factor(justice_clean$courtDirection);
justice_clean$justiceDirection <- factor(justice_clean$justiceDirection);

justice_clean %>% head()
```

Next we are trying to distilling important information from the score dataset. And after taking a look, it seems we only need a couple of the columns.
- term – the year the score was assigned
- justice – an ID that is unique to each Justice
- post_mn – the score of the Justice
- post_sd – the standard deviation of the score

So, like before we are going to filter out theses specific columns. Then we are going to rename justice, post_mn and post_sd to score_mean and score_sd, respectively, for personal clarity.

```{r, tidying_data_p2, 4}
justice_spread <-
  justice_idea %>%
  select(term, justiceID=justice, score_mean=post_mn, score_sd=post_sd)

justice_spread %>% head()
```

Finally, we are going to combine these two new concise datasets by using a inner join on term and justiceID. By doing this, we can create a new column in the new justice dataset that pertains to score and the standard deviation of said score.

```{r, joining_data, 4}
justice_combined <-
  justice_clean %>%
  inner_join(justice_spread)

justice_combined %>% head()
```

# **Data Analysis**

### Visualizing Data
At this point, we have cleaned our data. However, at this point the data is still numbers in a table. We need to convert it to visuals in order to truly to see if there is some sort of trend exists for us to continue.

So, we begin our visual analysis by graphing the total number of liberal decision by the Court and total number of conservative decisions by the Court over the years. Then we finish by drawing a regression lines over them to see any trends.
```{r, graph_p1, 6}
justice_combined %>%
  group_by(term, courtDirection) %>%
  summarize(no_of_votes=n()) %>%
  ggplot(mapping=aes(x=term, y=no_of_votes, color=courtDirection)) +
    geom_point() +
    geom_smooth()
```

Unfortunately, this analysis is neither easy to understand nor fruitful in anyway. Even though there is a downward trend, you have to realize the number of cases the Court took decreased over the years which could be the cause of the trend. This leads to the next theory that maybe by looking at each specific time periods of the data, we can determine something. In our case, we are going to split up the data based on the Chief Justice. In opinion, the best way would be create a function so, we can switch between Chief Justices without having to recode our pipeline. Also, it avoids cramping all the data together by displaying all of the groups.

```{r, graph_p2, 11}
eachCourt <- function(justice) {
  justice_combined %>%
    select(term, chief, justiceName, score_mean, score_sd) %>%
    filter(chief == justice) %>%
    distinct() %>%
    ggplot(mapping=aes(x=factor(term), y=score_mean,
      ymax=score_mean+score_sd, ymin=score_mean-score_sd, color=justiceName)) +
      geom_pointrange() +
      xlab("year") +
      ylab("average MQ score")
}

eachCourt("Vinson") # an older court
eachCourt("Roberts") # a newer court
```

While, this graph is slightly better than the previous one. This gives us an understanding of how more ideological an older Court like Vinson’s Court is versus a newer one like Robert’s Court. But still it isn’t giving us any sort of useful information that we want. So, maybe we need to go down to the level of each individual Justice to see a trend. I propose creating a visual based on splitting their votes over their entire tenure based on whether it is conservative or liberal.

```{r, graph_p3, 11}
eachJustice <- function(name) {
  justice_combined %>%
    select(term, justiceName, issueArea, justiceDirection) %>%
    filter(justiceName == name, !is.na(justice_combined$issueArea)) %>%
    ggplot(mapping=aes(x=factor(issueArea), y=justiceDirection, color=justiceDirection)) +
      geom_jitter() +
      theme(axis.text.x = element_text(angle=90)) +
      xlab("area of issue") +
      ylab("direction of vote")
}

eachJustice("AScalia") # a known conversative justice
eachJustice("RBGinsburg") # a known liberal justice
```

This graph is closer to what we want. We can see clear differences in certain issue areas between a liberal and conservative Justice. This is starting to give credence to the hypothesis that issue areas are correlated to overall ideological leaning. However, we are losing key information like how voting changed over their tenure in office. Also, the clutter makes it hard to tell at times which direction has more values. We can fix that by making a bar graph based on the count of the votes. However, this time we will graph over time and then facet the graph by issue area to gain a understanding of how voting changed over time on each issue.

```{r, graph_p4, 12}
eachJustice <- function(name) {
  justice_combined %>%
    select(term, justiceName, issueArea, justiceDirection) %>%
    filter(justiceName == name, !is.na(justice_combined$issueArea)) %>%
    ggplot(mapping=aes(x=factor(term), fill=justiceDirection)) +
      geom_bar() +
      facet_wrap(~issueArea, ncol=3, scales="free_y") +
      theme(axis.text.x = element_text(angle=90)) +
      xlab("year") +
      ylab("voting count")
}

eachJustice("AScalia") # a known conversative justice
eachJustice("RBGinsburg") # a known liberal justice
```

Finally, we have a graph is is helpful in guiding our understanding. It is clear that certain issues clearly delineate whether a Justice is conservative or not. For example, the third issue group, which deals with the First Amendment, has more conservative votes by conservative Justices and more liberal votes among liberal Justices. However, issue groups eleven, thirteen, and fourteen don’t seem as important because they so occur rarely and probably don’t have to be considered in our modeling. With this knowledge, we can move onto the coolest step: machine learning.

# **Predictive Model**

### Preprocessing Data
Before we can create our model, we need to do some preliminary work.

First, we are going to need to format our table in a more convenient manner; we need to make each row correspond to a particular observance. We do this by creating a table which states for each year and Justice, whether they were more conservative or liberal on a particular issue area. Then we call upon the spread function to make each issue area its own column. We make these columns’ values being whether the Justice for that year was more conservative or more liberal on that issue area. Then we inner join this new table with the classification we gain from the Martin and Quinn.  Finally, we factor the labels to make sure R considers them to categorical variables rather than numeric ones.

```{r, processing, 27}
justice_matrix <-
  justice_combined %>%
    group_by(justiceID, term, issueArea, justiceDirection) %>%
    summarize(directional_votes=n()) %>%
    ungroup() %>%
    inner_join(
      justice_combined %>%
        group_by(justiceID, term, issueArea) %>%
        summarise(total_votes=n()) %>%
        ungroup()
    ) %>%
  filter(!is.na(issueArea)) %>%
  mutate(direction=directional_votes/total_votes) %>%
  mutate(justiceID_year=paste(justiceID, "-", term)) %>%
  mutate(justiceDirection=ifelse(justiceDirection==0,1,2)) %>%
  group_by(justiceID_year, issueArea) %>%
  filter(direction == max(direction)) %>%
  filter(direction != 0.5) %>%
  ungroup() %>%
  select(justiceID_year, issueArea, justiceDirection) %>%
  spread(key=issueArea, value=justiceDirection) %>%
  replace(is.na(.),0) %>%
  inner_join(
    justice_spread %>%
      mutate(label=ifelse(score_mean > 0, 0, 1)) %>%
      mutate(justiceID_year=paste(justiceID, "-", term)) %>%
      select(justiceID_year, label)
  ) %>%
  select(-justiceID_year)

justice_matrix$label <- factor(justice_matrix$label);
```

Next, we are going to start my partitioning our dataset, so we may use part of it later for testing purposes.

```{r, partioning, 4}
set.seed(100)
indices <- createDataPartition(justice_matrix$label, p=0.70, list = F)
trainData <- justice_matrix[indices, ]
testData <- justice_matrix[-indices, ]
```

Then we are going to down sample our observance in order to have the same number of observances of liberal labels vs conservative labels. This is going to ensure our model doesn’t become biased to either one of the label over the other. Currently, our our observances stand at 259 observances for conservative vs 199 for liberal. However, with downsampling, we will have 199 observances for both conservatives and liberals

```{r, downsampling, 2}
set.seed(100)
justice_balanced <- downSample(x = trainData[, c(T,T,T,T,T,T,T,T,T,T,T,T,T,T,F)],
  y = factor(trainData$label));
```

### Creating Model
Now, with everything set we can turn our attention to creating our model.

For our model we are going to exclude the eleventh, thirteenth, and fourteenth group because our visual analysis shows it not to as significant as the other areas.

```{r, modeling_p1, 4}
model_one <- glm(Class~
  `1` + `10` + `12` + `2` + `3` + `4` + `5` + `6` + `7` + `8` + `9`,
  family='binomial', data=justice_balanced);

summary(model_one)
```

First of all, from this summary we can see the the model is a good fit from the deviance of residuals is close to zero. Second, we can see most of the areas are significant expect area four, six and seven. This means that we can reject the null hypothesis that there is no relationship between the areas and ideological leaning. However, lets try creating a new model but without the significant groups to see if we can improve our results.

```{r, modeling_p2, 4}
model_two <- glm(Class~
  `1` + `10` + `12` + `2` + `3` + `5` + `8` + `9`,
  family='binomial', data=justice_balanced);

summary(model_two)
```

Again, from this second summary, we the same good fit from the deviance of residuals. Moreover, this time all the areas are significant. Now, to test if this second model is a better fit than the first model.

```{r, good_fit_test, 1}
anova(model_one, model_two, test ="Chisq")
```

It seems based on the p-value that the second is model is not a significant and that means we fail to reject the null hypothesis. So, for now we will stick to the first model.

### Testing Model
Lets test our model to see what the accuracy of our model is by using the test data that we procured before.

```{r, testing, 5}
predictions <- predict(model_one, newdata = testData, type = "response")
classify <- ifelse(predictions >= 0.50, 1, 0)
classify <- factor(classify)
accuracy <- mean(classify == testData$label)
```

It seems our model can predict with an accuracy of `r round(accuracy * 100, digits = 2)`%

# **Conclusion**

### Final Thoughts
Overall, we can see that by using how nominees have acted in certain areas in the past, we can determine with a 90% accuracy whether or not they are left leaning or right leaning. It means that Senators can use something similar to this model to figure out whether a nominee truly believe what they say they believe.

### Improvements
Some ways that we could potentially improve on this model is include by more including more groups like independent, moderately liberal, and moderately conservative. This would help to show a difference between someone who is only slight right leaning vs someone who is extremely conservative.

### Further Resources
If you are interested in further looking at the topic of Supreme Court nominations, I suggest you take a look at [this website](https://aflcio.org/2016/8/30/why-supreme-court-nominations-are-one-most-important-issues-working-people) for a starting place. Also, I also suggest you read [this journal entry](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1000986) because it this the same entry that allowed me to find certain weaknesses in the Martin-Quinn Score. Finally, if you are interested in improving on this model based on my suggested improvements, I suggest you visit [this tutorial](https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/) from UCLA that takes you step-by-step on how to do multi class logistic regression, which is also done in R.

### Citations
- [Case/Justice Dataset](http://scdb.wustl.edu/data.php)
- [Case/Justice Handbook](http://scdb.wustl.edu/documentation.php?s=2)
- [Martin-Quinn Dataset and Handbook](https://mqscores.lsa.umich.edu/measures.php)
